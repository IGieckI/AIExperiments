{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f7988dd1",
      "metadata": {
        "id": "f7988dd1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c868dedd",
      "metadata": {
        "id": "c868dedd"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_token, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_token % num_heads == 0, \"d_token must be divisible by num_heads\"\n",
        "\n",
        "        self.d_token = d_token\n",
        "        self.num_heads = num_heads\n",
        "        # Dimension of each head\n",
        "        self.d_k = d_token // num_heads\n",
        "        self.square_d_k = math.sqrt(self.d_k)\n",
        "\n",
        "        self.W_q = nn.Linear(d_token, d_token)\n",
        "        self.W_k = nn.Linear(d_token, d_token)\n",
        "        self.W_v = nn.Linear(d_token, d_token)\n",
        "\n",
        "        self.W_o = nn.Linear(d_token, d_token)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Linear projections to get Q, K, V with reshape to (Batch, Seq_Len, Num_Heads, Head_Dim)\n",
        "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.square_d_k\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Weighted sum of values (Batch, Heads, Seq_Len, Head_Dim)\n",
        "        context = torch.matmul(attn_weights, V)\n",
        "\n",
        "        # Concatenate heads and apply final linear layer + reshape back to (Batch, Seq_Len, d_token)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_token)\n",
        "        output = self.W_o(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_token, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_token, num_heads)\n",
        "\n",
        "        # Layer Norms\n",
        "        self.norm1 = nn.LayerNorm(d_token)\n",
        "        self.norm2 = nn.LayerNorm(d_token)\n",
        "\n",
        "        # Feed Forward Network\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_token, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_token)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.attention(x, mask)\n",
        "        # Residual connection\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "\n",
        "        # Feed Forward + Residual + Norm\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_token, num_heads, num_blocks, d_ff, max_seq_len):\n",
        "        super().__init__()\n",
        "        # Token Embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, d_token)\n",
        "\n",
        "        # Positional Embeddings\n",
        "        self.position_embedding = nn.Embedding(max_seq_len, d_token)\n",
        "\n",
        "        # Stack of Transformer Blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(d_token, num_heads, d_ff) for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "        # Unembedding Layer\n",
        "        self.lm_head = nn.Linear(d_token, vocab_size, bias=False)\n",
        "        self.lm_head.weight = self.embedding.weight\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.shape\n",
        "\n",
        "        # Causal mask\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len)).to(x.device)\n",
        "\n",
        "        # Create position indices (0, 1, 2, ..., seq_len-1)\n",
        "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
        "        x = self.embedding(x) + self.position_embedding(positions)\n",
        "\n",
        "        # Pass through all transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, mask)\n",
        "\n",
        "        # Final projection to vocabulary size\n",
        "        logits = self.lm_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8be23b49",
      "metadata": {
        "id": "8be23b49"
      },
      "source": [
        "### Data Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2480c64c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2480c64c",
        "outputId": "93996f35-0ebc-4f50-9549-72ba29c8efed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size: 29\n",
            "Sample vocabulary: [('a', 3), ('b', 4), ('c', 5), ('d', 6), ('e', 7)]\n"
          ]
        }
      ],
      "source": [
        "with open('./names.txt', 'r') as f:\n",
        "\tnames = f.read().splitlines()\n",
        "raw_names = names[:10000]\n",
        "\n",
        "# Create Vocabulary (Character-level)\n",
        "chars = sorted(list(set(\"\".join(raw_names))))\n",
        "# Add special tokens:\n",
        "# <PAD>: Padding (0)\n",
        "# <SOS>: Start of Sequence (1)\n",
        "# <EOS>: End of Sequence (2)\n",
        "stoi = {ch: i+3 for i, ch in enumerate(chars)}\n",
        "stoi['<PAD>'] = 0\n",
        "stoi['<SOS>'] = 1\n",
        "stoi['<EOS>'] = 2\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "vocab_size = len(stoi)\n",
        "\n",
        "print(f\"Vocab Size: {vocab_size}\")\n",
        "print(f\"Sample vocabulary: {list(stoi.items())[:5]}\")\n",
        "\n",
        "max_len = max(len(name) for name in raw_names) + 2 # +2 for <SOS> and <EOS>\n",
        "\n",
        "def encode_name(name):\n",
        "    # Convert name to integers with <SOS> and <EOS>\n",
        "    return [stoi['<SOS>']] + [stoi[ch] for ch in name] + [stoi['<EOS>']]\n",
        "\n",
        "# Prepare Training Data (Pad to max_len)\n",
        "X_train = []\n",
        "Y_train = []\n",
        "\n",
        "for name in raw_names:\n",
        "    encoded = encode_name(name)\n",
        "    # <SOS> A l i c e -> A l i c e <EOS>\n",
        "    input_seq = encoded[:-1]\n",
        "    target_seq = encoded[1:]\n",
        "\n",
        "    # Pad sequences to constant length\n",
        "    pad_len = max_len - len(input_seq)\n",
        "    input_seq += [stoi['<PAD>']] * pad_len\n",
        "    target_seq += [stoi['<PAD>']] * pad_len\n",
        "\n",
        "    X_train.append(input_seq)\n",
        "    Y_train.append(target_seq)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.long)\n",
        "Y_train = torch.tensor(Y_train, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc8c2c14",
      "metadata": {
        "id": "bc8c2c14"
      },
      "source": [
        "### Training + Testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_name(start_str):\n",
        "    model.eval()\n",
        "    context = [stoi['<SOS>']] + [stoi.get(c, 0) for c in start_str]\n",
        "    context = torch.tensor(context, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    generated_name = start_str\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        if context.shape[1] >= max_len:\n",
        "            break\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(context)\n",
        "            # Look only at the last predicted token\n",
        "            last_logits = logits[:, -1, :]\n",
        "\n",
        "            # Sample from the distribution\n",
        "            probs = F.softmax(last_logits, dim=-1)\n",
        "            # Prevent generating <SOS> or <PAD>\n",
        "            probs[0][stoi['<SOS>']] = 0\n",
        "            probs[0][stoi['<PAD>']] = 0\n",
        "\n",
        "            # Pick next char\n",
        "            next_ix = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            if next_ix == stoi['<EOS>']:\n",
        "                break\n",
        "\n",
        "            generated_name += itos[next_ix]\n",
        "\n",
        "            # Append to context for next step\n",
        "            context = torch.cat([context, torch.tensor([[next_ix]])], dim=1)\n",
        "\n",
        "    return generated_name"
      ],
      "metadata": {
        "id": "asSt34uH1zaG"
      },
      "id": "asSt34uH1zaG",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "14adc711",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14adc711",
        "outputId": "4d5924f5-b826-479a-cfd8-488cf171328e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 20, Loss: 3.7929\n",
            "Input 'A': Aeaaliyyyaleno\n",
            "Input 'Ma': Malynegke\n",
            "Input 'Z': Zle\n",
            "Input 'Ca': Caaag\n",
            "Epoch 40, Loss: 2.6267\n",
            "Input 'A': Amlza\n",
            "Input 'Ma': Maraieh\n",
            "Input 'Z': Zaaaihijan\n",
            "Input 'Ca': Cairlyn\n",
            "Epoch 60, Loss: 2.5284\n",
            "Input 'A': Aglidne\n",
            "Input 'Ma': Mavtmya\n",
            "Input 'Z': Zatiih\n",
            "Input 'Ca': Calalhi\n",
            "Epoch 80, Loss: 2.4755\n",
            "Input 'A': Aiaai\n",
            "Input 'Ma': Maeeia\n",
            "Input 'Z': Zeaa\n",
            "Input 'Ca': Caa\n",
            "Epoch 100, Loss: 2.4322\n",
            "Input 'A': Ayliane\n",
            "Input 'Ma': Maarr\n",
            "Input 'Z': Zoaay\n",
            "Input 'Ca': Caaka\n",
            "Epoch 120, Loss: 2.3678\n",
            "Input 'A': Airien\n",
            "Input 'Ma': Marlah\n",
            "Input 'Z': Zrmea\n",
            "Input 'Ca': Caeeeninh\n",
            "Epoch 140, Loss: 2.2872\n",
            "Input 'A': Aialela\n",
            "Input 'Ma': Mananer\n",
            "Input 'Z': Zelada\n",
            "Input 'Ca': Caealis\n",
            "Epoch 160, Loss: 2.2232\n",
            "Input 'A': Aenicryna\n",
            "Input 'Ma': Mabmiuh\n",
            "Input 'Z': Zaseniha\n",
            "Input 'Ca': Calilin\n",
            "Epoch 180, Loss: 2.1694\n",
            "Input 'A': Aaivey\n",
            "Input 'Ma': Maeynn\n",
            "Input 'Z': Zionla\n",
            "Input 'Ca': Canolyn\n",
            "Epoch 200, Loss: 2.1314\n",
            "Input 'A': Ayanni\n",
            "Input 'Ma': Maomaha\n",
            "Input 'Z': Zaluay\n",
            "Input 'Ca': Caurli\n"
          ]
        }
      ],
      "source": [
        "d_token = 32\n",
        "num_heads = 4\n",
        "num_blocks = 2\n",
        "d_ff = 64\n",
        "learning_rate = 0.005\n",
        "epochs = 200\n",
        "\n",
        "model = Transformer(vocab_size, d_token, num_heads, num_blocks, d_ff, max_len)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# Ensure the loss is not calculated on <PAD> tokens (id 0)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(X_train)\n",
        "\n",
        "    loss = criterion(logits.view(-1, vocab_size), Y_train.view(-1))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "        print(\"Input 'A':\", generate_name(\"A\"))\n",
        "        print(\"Input 'Ma':\", generate_name(\"Ma\"))\n",
        "        print(\"Input 'Z':\", generate_name(\"Z\"))\n",
        "        print(\"Input 'Ca':\", generate_name(\"Ca\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2iA_XWqF11qE"
      },
      "id": "2iA_XWqF11qE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}