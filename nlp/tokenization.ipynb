{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a7a7c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "from typing import Dict, List, Tuple, Set\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d236d5",
   "metadata": {},
   "source": [
    "## 1. BPE (Byte Pair Encoding) Tokenizer\n",
    "\n",
    "BPE is a data-driven subword tokenization algorithm that iteratively merges the most frequent pair of consecutive symbols in a corpus. It starts with a vocabulary of individual characters and gradually builds up larger subword units.\n",
    "\n",
    "**Function-based implementation** using global variables to maintain state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64612f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_vocab = {}\n",
    "bpe_merges = []\n",
    "bpe_word_freqs = {}\n",
    "bpe_vocab_size = 1000\n",
    "\n",
    "def get_pair_stats(splits, word_freqs):\n",
    "    \"\"\"Get frequency of consecutive symbol pairs\"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        symbols = splits[word]\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab_pair(pair, splits):\n",
    "    \"\"\"Merge the most frequent pair in the vocabulary\"\"\"\n",
    "    new_splits = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    for word in splits:\n",
    "        new_word = p.sub(''.join(pair), ' '.join(splits[word]))\n",
    "        new_splits[word] = new_word.split()\n",
    "    return new_splits\n",
    "\n",
    "def train_bpe(corpus, vocab_size=1000):\n",
    "    \"\"\"\n",
    "    Train the BPE tokenizer on a given corpus\n",
    "    \"\"\"\n",
    "    global bpe_vocab, bpe_merges, bpe_word_freqs, bpe_vocab_size\n",
    "    \n",
    "    bpe_vocab_size = vocab_size\n",
    "    bpe_word_freqs = collections.defaultdict(int)\n",
    "    bpe_merges = []\n",
    "    \n",
    "    # Count word frequencies\n",
    "    for text in corpus:\n",
    "        words = text.lower().split()\n",
    "        for word in words:\n",
    "            bpe_word_freqs[word] += 1\n",
    "    \n",
    "    # Initialize splits, each word split into characters with </w> at the end\n",
    "    splits = {}\n",
    "    for word in bpe_word_freqs:\n",
    "        splits[word] = list(word) + ['</w>']\n",
    "    \n",
    "    # Get initial vocabulary (all characters)\n",
    "    vocab = set()\n",
    "    for word in bpe_word_freqs:\n",
    "        vocab.update(splits[word])\n",
    "    \n",
    "    # Convert to numbered vocabulary\n",
    "    bpe_vocab = {symbol: i for i, symbol in enumerate(sorted(vocab))}\n",
    "    \n",
    "    # Perform BPE merges\n",
    "    num_merges = bpe_vocab_size - len(bpe_vocab)\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        pairs = get_pair_stats(splits, bpe_word_freqs)\n",
    "        if not pairs:\n",
    "            break\n",
    "            \n",
    "        # Find most frequent pair\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        \n",
    "        # Merge the pair\n",
    "        splits = merge_vocab_pair(best_pair, splits)\n",
    "        \n",
    "        # Add merged token to vocabulary\n",
    "        new_token = ''.join(best_pair)\n",
    "        bpe_vocab[new_token] = len(bpe_vocab)\n",
    "        bpe_merges.append(best_pair)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Merge {i}: {best_pair} -> {new_token}\")\n",
    "\n",
    "def apply_bpe_merges(word):\n",
    "    \"\"\"Apply learned merges to a word\"\"\"\n",
    "    splits = list(word) + ['</w>']\n",
    "    \n",
    "    for merge in bpe_merges:\n",
    "        i = 0\n",
    "        while i < len(splits) - 1:\n",
    "            if splits[i] == merge[0] and splits[i + 1] == merge[1]:\n",
    "                splits = splits[:i] + [''.join(merge)] + splits[i + 2:]\n",
    "            else:\n",
    "                i += 1\n",
    "    return splits\n",
    "\n",
    "def tokenize_bpe(text):\n",
    "    \"\"\"\n",
    "    Tokenize text using learned BPE\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    tokens = []\n",
    "    \n",
    "    for word in words:\n",
    "        word_tokens = apply_bpe_merges(word)\n",
    "        tokens.extend(word_tokens)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def encode_bpe(text):\n",
    "    \"\"\"\n",
    "    Encode text to token IDs\n",
    "    \"\"\"\n",
    "    tokens = tokenize_bpe(text)\n",
    "    return [bpe_vocab.get(token, bpe_vocab.get('<unk>', 0)) for token in tokens]\n",
    "\n",
    "def decode_bpe(token_ids):\n",
    "    \"\"\"\n",
    "    Decode token IDs back to text\n",
    "    \"\"\"\n",
    "    # Create reverse vocabulary\n",
    "    id_to_token = {v: k for k, v in bpe_vocab.items()}\n",
    "    \n",
    "    tokens = [id_to_token.get(token_id, '<unk>') for token_id in token_ids]\n",
    "    \n",
    "    # Join tokens and handle word boundaries\n",
    "    text = ''.join(tokens).replace('</w>', ' ').strip()\n",
    "    return text\n",
    "\n",
    "def save_bpe_model(filepath):\n",
    "    \"\"\"Save the trained BPE model\"\"\"\n",
    "    model_data = {\n",
    "        'vocab': bpe_vocab,\n",
    "        'merges': bpe_merges,\n",
    "        'vocab_size': bpe_vocab_size\n",
    "    }\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(model_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def load_bpe_model(filepath):\n",
    "    \"\"\"Load a trained BPE model\"\"\"\n",
    "    global bpe_vocab, bpe_merges, bpe_vocab_size\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        model_data = json.load(f)\n",
    "    \n",
    "    bpe_vocab = model_data['vocab']\n",
    "    bpe_merges = [tuple(merge) for merge in model_data['merges']]\n",
    "    bpe_vocab_size = model_data['vocab_size']\n",
    "\n",
    "def get_bpe_vocab_size():\n",
    "    \"\"\"Get current BPE vocabulary size\"\"\"\n",
    "    return len(bpe_vocab)\n",
    "\n",
    "def get_bpe_merges_count():\n",
    "    \"\"\"Get number of BPE merges learned\"\"\"\n",
    "    return len(bpe_merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14538256",
   "metadata": {},
   "source": [
    "## Rule-based Tokenizer\n",
    "\n",
    "A rule-based tokenizer uses predefined rules and patterns to split text into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fad5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for rule-based tokenizer configuration\n",
    "rule_preserve_case = False\n",
    "rule_handle_contractions = True\n",
    "rule_handle_punctuation = True\n",
    "rule_handle_numbers = True\n",
    "rule_handle_urls = True\n",
    "rule_handle_emails = True\n",
    "rule_pattern = None\n",
    "\n",
    "# Common contractions mapping\n",
    "contractions_map = {\n",
    "    \"n't\": \"not\",\n",
    "    \"'re\": \"are\", \n",
    "    \"'ve\": \"have\",\n",
    "    \"'ll\": \"will\",\n",
    "    \"'d\": \"would\",\n",
    "    \"'m\": \"am\",\n",
    "    \"'s\": \"is\"  # Note: 's can also be possessive\n",
    "}\n",
    "\n",
    "def configure_rule_tokenizer(preserve_case=False,\n",
    "                           handle_contractions=True,\n",
    "                           handle_punctuation=True,\n",
    "                           handle_numbers=True,\n",
    "                           handle_urls=True,\n",
    "                           handle_emails=True):\n",
    "    \"\"\"\n",
    "    Configure rule-based tokenizer with various options\n",
    "    \n",
    "\tpreserve_case: Whether to preserve original casing\n",
    "\thandle_contractions: Whether to split contractions (e.g., \"don't\" -> \"do\", \"n't\")\n",
    "\thandle_punctuation: Whether to separate punctuation\n",
    "\thandle_numbers: Whether to treat numbers as separate tokens\n",
    "\thandle_urls: Whether to keep URLs as single tokens\n",
    "\thandle_emails: Whether to keep emails as single tokens\n",
    "    \"\"\"\n",
    "    global rule_preserve_case, rule_handle_contractions, rule_handle_punctuation\n",
    "    global rule_handle_numbers, rule_handle_urls, rule_handle_emails, rule_pattern\n",
    "    \n",
    "    rule_preserve_case = preserve_case\n",
    "    rule_handle_contractions = handle_contractions\n",
    "    rule_handle_punctuation = handle_punctuation\n",
    "    rule_handle_numbers = handle_numbers\n",
    "    rule_handle_urls = handle_urls\n",
    "    rule_handle_emails = handle_emails\n",
    "    \n",
    "    # Compile patterns\n",
    "    rule_pattern = compile_rule_patterns()\n",
    "\n",
    "def compile_rule_patterns():\n",
    "    \"\"\"Compile regex patterns for tokenization\"\"\"\n",
    "    patterns = []\n",
    "    \n",
    "    # URLs\n",
    "    if rule_handle_urls:\n",
    "        url_pattern = r'https?://(?:[-\\w.])+(?:[:\\d]+)?(?:/(?:[\\w/_.])*(?:\\?(?:[\\w&=%.])*)?(?:#(?:\\w)*)?)?'\n",
    "        patterns.append(url_pattern)\n",
    "    \n",
    "    # Email addresses\n",
    "    if rule_handle_emails:\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        patterns.append(email_pattern)\n",
    "    \n",
    "    # Numbers\n",
    "    if rule_handle_numbers:\n",
    "        number_pattern = r'\\d+(?:\\.\\d+)?%?'\n",
    "        patterns.append(number_pattern)\n",
    "    \n",
    "    # Contractions\n",
    "    if rule_handle_contractions:\n",
    "        contraction_pattern = r\"\\w+(?:'[a-z]+)?\"\n",
    "        patterns.append(contraction_pattern)\n",
    "    \n",
    "    # Words (letters, including apostrophes within words)\n",
    "    patterns.append(r\"\\w+(?:'\\w+)*\")\n",
    "    \n",
    "    # Punctuation\n",
    "    if rule_handle_punctuation:\n",
    "        patterns.append(r'[^\\w\\s]')\n",
    "    \n",
    "    # Combine all patterns\n",
    "    return re.compile('|'.join(f'({p})' for p in patterns), re.IGNORECASE)\n",
    "\n",
    "def handle_contraction(token):\n",
    "    \"\"\"Handle contraction splitting\"\"\"\n",
    "    if not rule_handle_contractions:\n",
    "        return [token]\n",
    "    \n",
    "    # Check for common contractions\n",
    "    for contraction, expansion in contractions_map.items():\n",
    "        if token.lower().endswith(contraction):\n",
    "            base = token[:-len(contraction)]\n",
    "            if base:\n",
    "                return [base, expansion]\n",
    "    \n",
    "    return [token]\n",
    "\n",
    "def normalize_case(token):\n",
    "    \"\"\"Normalize case based on settings\"\"\"\n",
    "    if rule_preserve_case:\n",
    "        return token\n",
    "    return token.lower()\n",
    "\n",
    "def tokenize_rule_based(text):\n",
    "    \"\"\"\n",
    "    Tokenize text using rule-based approach\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    if rule_pattern is None:\n",
    "        raise ValueError(\"Rule tokenizer not configured. Call configure_rule_tokenizer() first.\")\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    # Find all matches using compiled pattern\n",
    "    for match in rule_pattern.finditer(text):\n",
    "        token = match.group().strip()\n",
    "        if not token:\n",
    "            continue\n",
    "        \n",
    "        # Handle contractions\n",
    "        if rule_handle_contractions and \"'\" in token:\n",
    "            contraction_tokens = handle_contraction(token)\n",
    "            for ct in contraction_tokens:\n",
    "                tokens.append(normalize_case(ct))\n",
    "        else:\n",
    "            tokens.append(normalize_case(token))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def sentence_tokenize(text):\n",
    "    \"\"\"\n",
    "    Split text into sentences using rules\n",
    "    \"\"\"\n",
    "    # Pattern for sentence boundaries\n",
    "    sentence_pattern = r'(?<=[.!?])\\s+(?=[A-Z])'\n",
    "    \n",
    "    # Handle abbreviations (simple approach)\n",
    "    # Replace common abbreviations temporarily\n",
    "    abbrevs = ['Mr.', 'Mrs.', 'Dr.', 'Prof.', 'Sr.', 'Jr.', 'vs.', 'etc.', 'i.e.', 'e.g.']\n",
    "    temp_text = text\n",
    "    for i, abbrev in enumerate(abbrevs):\n",
    "        temp_text = temp_text.replace(abbrev, f'__ABBREV_{i}__')\n",
    "    \n",
    "    # Split sentences\n",
    "    sentences = re.split(sentence_pattern, temp_text)\n",
    "    \n",
    "    # Restore abbreviations\n",
    "    for i, abbrev in enumerate(abbrevs):\n",
    "        sentences = [s.replace(f'__ABBREV_{i}__', abbrev) for s in sentences]\n",
    "    \n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "def get_token_types(tokens):\n",
    "    \"\"\"\n",
    "    Classify tokens by type\n",
    "    \"\"\"\n",
    "    types = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if re.match(r'https?://', token):\n",
    "            types.append('URL')\n",
    "        elif re.match(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', token):\n",
    "            types.append('EMAIL')\n",
    "        elif re.match(r'\\d+(?:\\.\\d+)?%?', token):\n",
    "            types.append('NUMBER')\n",
    "        elif re.match(r'[^\\w\\s]', token):\n",
    "            types.append('PUNCTUATION')\n",
    "        elif token.lower() in contractions_map.values():\n",
    "            types.append('CONTRACTION_WORD')\n",
    "        elif re.match(r'\\w+', token):\n",
    "            types.append('WORD')\n",
    "        else:\n",
    "            types.append('OTHER')\n",
    "    \n",
    "    return types\n",
    "\n",
    "def get_tokenization_statistics(text):\n",
    "    \"\"\"\n",
    "    Get tokenization statistics\n",
    "    \"\"\"\n",
    "    tokens = tokenize_rule_based(text)\n",
    "    token_types = get_token_types(tokens)\n",
    "    \n",
    "    stats = {\n",
    "        'total_tokens': len(tokens),\n",
    "        'unique_tokens': len(set(tokens)),\n",
    "        'sentences': len(sentence_tokenize(text)),\n",
    "        'characters': len(text),\n",
    "        'characters_no_spaces': len(text.replace(' ', ''))\n",
    "    }\n",
    "    \n",
    "    # Count by token type\n",
    "    type_counts = collections.Counter(token_types)\n",
    "    stats.update(type_counts)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65c289f",
   "metadata": {},
   "source": [
    "## 3. Example Usage and Comparison\n",
    "\n",
    "Let's test both tokenizers with sample text to see how they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd413573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training corpus for BPE\n",
    "training_corpus = [\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"natural language processing is fascinating\",\n",
    "    \"machine learning algorithms are powerful tools\",\n",
    "    \"tokenization is an important preprocessing step\",\n",
    "    \"byte pair encoding learns subword units automatically\",\n",
    "    \"rule based approaches use predefined patterns\",\n",
    "    \"both methods have their advantages and disadvantages\",\n",
    "    \"preprocessing text data requires careful consideration\",\n",
    "    \"the effectiveness of tokenization depends on the task\",\n",
    "    \"subword tokenization helps with out of vocabulary words\"\n",
    "]\n",
    "\n",
    "# Test text\n",
    "test_text = \"\"\"\n",
    "Hello world! There can't be cats in my house otherwise it's gonna explode or something. \n",
    "Stuff stuff, https://www.google.com, giacomo.antonelli3@gmail.com at $0.99 if interested, possible 99% discount!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a58fe5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 0: ('s', '</w>') -> s</w>\n",
      "Merge 100: ('langu', 'ag') -> languag\n",
      "\n",
      "BPE Vocabulary size: 200\n",
      "Number of merges learned: 173\n",
      "\n",
      "BPE Tokens (164): ['h', 'e', 'l', 'l', 'o', '</w>', 'wor', 'l', 'd', '!', '</w>', 'th', 'er', 'e</w>', 'c', 'an', \"'\", 't</w>', 'b', 'e</w>', 'c', 'at', 's</w>', 'in', '</w>', 'm', 'y</w>', 'h', 'o', 'use</w>', 'o', 'th', 'er', 'w', 'i', 's', 'e</w>', 'i', 't', \"'\", 's</w>', 'g', 'o', 'n', 'n', 'a', '</w>', 'e', 'x', 'p', 'l', 'o', 'd', 'e</w>', 'or', '</w>', 's', 'o', 'm', 'e', 'th', 'in', 'g', '.', '</w>', 's', 't', 'u', 'f', 'f', '</w>', 's', 't', 'u', 'f', 'f', ',', '</w>', 'h', 't', 't', 'p', 's', ':', '/', '/', 'w', 'w', 'w', '.', 'g', 'o', 'o', 'g', 'le', '.', 'co', 'm', ',', '</w>', 'g', 'i', 'ac', 'o', 'm', 'o', '.', 'an', 'to', 'n', 'e', 'l', 'l', 'i', '3', '@', 'g', 'm', 'a', 'i', 'l', '.', 'co', 'm', '</w>', 'at', '</w>', '$', '0', '.', '9', '9', '</w>', 'i', 'f', '</w>', 'in', 't', 'er', 'es', 'te', 'd', ',', '</w>', 'p', 'o', 's', 's', 'i', 'b', 'l', 'e</w>', '9', '9', '%', '</w>', 'd', 'i', 's', 'co', 'un', 't', '!', '</w>']\n",
      "BPE Decoded: hello world  there can t be cats in my house otherwise it s gonna explode or something  stuff stuff  https   www google com  giacomo antonelli  gmail com at       if interested  possible     discount\n"
     ]
    }
   ],
   "source": [
    "# Training and use BPE\n",
    "train_bpe(training_corpus, vocab_size=200)\n",
    "\n",
    "print(f\"\\nBPE Vocabulary size: {get_bpe_vocab_size()}\")\n",
    "print(f\"Number of merges learned: {get_bpe_merges_count()}\")\n",
    "\n",
    "bpe_tokens = tokenize_bpe(test_text)\n",
    "print(f\"\\nBPE Tokens ({len(bpe_tokens)}): {bpe_tokens}\")\n",
    "\n",
    "bpe_encoded = encode_bpe(test_text)\n",
    "bpe_decoded = decode_bpe(bpe_encoded)\n",
    "print(f\"BPE Decoded: {bpe_decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f214fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule-based Tokens (35): ['Hello', 'world', '!', 'There', 'ca', 'not', 'be', 'cats', 'in', 'my', 'house', 'otherwise', 'it', 'is', 'gonna', 'explode', 'or', 'something', '.', 'Stuff', 'stuff', ',', 'https://www.google.com', ',', 'giacomo.antonelli3@gmail.com', 'at', '$', '0.99', 'if', 'interested', ',', 'possible', '99%', 'discount', '!']\n",
      "\n",
      "Token types: ['WORD', 'WORD', 'PUNCTUATION', 'WORD', 'WORD', 'CONTRACTION_WORD', 'WORD', 'WORD', 'WORD', 'WORD', 'WORD', 'WORD', 'WORD', 'CONTRACTION_WORD', 'WORD', 'WORD', 'WORD', 'WORD', 'PUNCTUATION', 'WORD', 'WORD', 'PUNCTUATION', 'URL', 'PUNCTUATION', 'EMAIL', 'WORD', 'PUNCTUATION', 'NUMBER', 'WORD', 'WORD', 'PUNCTUATION', 'WORD', 'NUMBER', 'WORD', 'PUNCTUATION']\n",
      "\n",
      "Tokenization statistics:\n",
      "  total_tokens: 35\n",
      "  unique_tokens: 32\n",
      "  sentences: 3\n",
      "  characters: 203\n",
      "  characters_no_spaces: 178\n",
      "  WORD: 22\n",
      "  PUNCTUATION: 7\n",
      "  CONTRACTION_WORD: 2\n",
      "  URL: 1\n",
      "  EMAIL: 1\n",
      "  NUMBER: 2\n",
      "\n",
      "Sentences (3):\n",
      "  1. Hello world!\n",
      "  2. There can't be cats in my house otherwise it's gonna explode or something.\n",
      "  3. Stuff stuff, https://www.google.com, giacomo.antonelli3@gmail.com at $0.99 if interested, possible 99% discount!\n"
     ]
    }
   ],
   "source": [
    "# Configuration and use of rule-based tokenizer\n",
    "configure_rule_tokenizer(\n",
    "    preserve_case=True,\n",
    "    handle_contractions=True,\n",
    "    handle_punctuation=True,\n",
    "    handle_numbers=True,\n",
    "    handle_urls=True,\n",
    "    handle_emails=True\n",
    ")\n",
    "\n",
    "rule_tokens = tokenize_rule_based(test_text)\n",
    "print(f\"Rule-based Tokens ({len(rule_tokens)}): {rule_tokens}\")\n",
    "\n",
    "token_types = get_token_types(rule_tokens)\n",
    "print(f\"\\nToken types: {token_types}\")\n",
    "\n",
    "stats = get_tokenization_statistics(test_text)\n",
    "print(f\"\\nTokenization statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "sentences = sentence_tokenize(test_text)\n",
    "print(f\"\\nSentences ({len(sentences)}):\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"  {i}. {sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
